from __future__ import annotations

import argparse
import logging
import sys
from pathlib import Path

import pandas as pd

import argparse
import logging

import pandas as pd

from .config import load_config
from .db import get_statistics, init_db, read_papers, save_papers
from .database.manager import DatabaseManager
from .pipeline.run import SystematicReviewPipeline

logger = logging.getLogger(__name__)

# Garantir UTF-8 no Windows - m√©todo alternativo que n√£o quebra logging
if sys.platform == "win32":
    # Configurar vari√°vel de ambiente antes de qualquer print
    import os
    os.environ["PYTHONIOENCODING"] = "utf-8"
    # Reconfigurar stdout/stderr se ainda n√£o estiver em UTF-8
    if hasattr(sys.stdout, 'reconfigure'):
        sys.stdout.reconfigure(encoding='utf-8', errors='replace')
        sys.stderr.reconfigure(encoding='utf-8', errors='replace')


def cmd_init_db(_: argparse.Namespace) -> None:
    cfg = load_config()
    db_path = init_db(cfg)
    print(f"Banco inicializado em: {db_path}")


def cmd_show(_: argparse.Namespace) -> None:
    cfg = load_config()
    df = read_papers(cfg)
    print(df.head(20).to_string(index=False))
    print(f"\nTotal de registros: {len(df)}")


def cmd_import_csv(ns: argparse.Namespace) -> None:
    cfg = load_config()
    csv_path = Path(ns.csv)
    if not csv_path.exists():
        raise SystemExit(f"Arquivo CSV n√£o encontrado: {csv_path}")
    df = pd.read_csv(csv_path)
    inserted = save_papers(df, cfg)
    print(f"Inseridos {inserted} novos registros a partir de {csv_path}")


def cmd_stats(_: argparse.Namespace) -> None:
    cfg = load_config()
    stats = get_statistics(cfg)
    
    print("üìä Estat√≠sticas do Banco de Dados")
    print("=" * 40)
    print(f"Total de papers: {stats['total_papers']}")
    
    if stats.get('by_stage'):
        print("\nüìã Por est√°gio de sele√ß√£o:")
        label_map = {
            'identification': 'Identification',
            'screening_excluded': 'Screening Excluded',
            'eligibility_excluded': 'Eligibility Excluded',
            'included': 'Included'
        }
        for stage, count in stats['by_stage'].items():
            print(f"  {label_map.get(stage, stage)}: {count}")
    
    if stats.get('by_database'):
        print("\nüóÉÔ∏è Por base de dados:")
        for db, count in stats['by_database'].items():
            print(f"  {db}: {count}")
    
    if stats.get('by_year'):
        print("\nüìÖ Por ano (√∫ltimos 10):")
        for year, count in stats['by_year'].items():
            print(f"  {year}: {count}")
    
    if stats.get('cache'):
        cache_stats = stats['cache']
        print(f"\nüíæ Cache: {cache_stats['total_entries']} entradas, {cache_stats['total_hits']} hits")


def cmd_run_pipeline(ns: argparse.Namespace) -> None:
    cfg = load_config()
    pipeline = SystematicReviewPipeline(cfg)
    
    # Configura√ß√µes personalizadas
    apis = getattr(ns, 'apis', ["semantic_scholar", "openalex", "crossref", "core"])
    limit_per_query = getattr(ns, 'limit_per_query', 50)
    min_score = getattr(ns, 'min_score', 4.0)
    
    print(f"üîß Configura√ß√£o:")
    print(f"  APIs: {', '.join(apis)}")
    print(f"  Limite por query: {limit_per_query}")
    print(f"  Score m√≠nimo: {min_score}")
    
    # Executar pipeline com configura√ß√µes
    try:
        # Execu√ß√£o passo a passo para usar configura√ß√µes customizadas
        pipeline.generate_search_queries()
        pipeline.collect_data(apis=apis, limit_per_query=limit_per_query)
        pipeline.process_data()
        pipeline.apply_selection_criteria(min_score)
        
        # ‚úÖ CORRE√á√ÉO: Salvar papers no banco ap√≥s sele√ß√£o PRISMA
        saved = save_papers(pipeline.results, cfg)
        print(f"üíæ Salvos {saved} papers no banco de dados")
        
        # ‚úÖ CORRE√á√ÉO: Persistir an√°lises usando IDs reais dos papers salvos
        try:
            db_manager = DatabaseManager()
            analysis_count = 0
            
            # Buscar papers salvos com DOI/t√≠tulo para mapear IDs reais
            import sqlite3
            conn = sqlite3.connect('research/systematic_review.db')
            cursor = conn.cursor()
            
            for idx, row in pipeline.results.iterrows():
                if pd.notna(row.get('relevance_score')):
                    # Buscar ID real do paper usando DOI ou t√≠tulo
                    doi = row.get('doi')
                    title = row.get('title', '')
                    
                    paper_id = None
                    if doi:
                        cursor.execute('SELECT id FROM papers WHERE doi = ? LIMIT 1', (doi,))
                        result = cursor.fetchone()
                        if result:
                            paper_id = result[0]
                    
                    if not paper_id and title:
                        cursor.execute('SELECT id FROM papers WHERE title = ? LIMIT 1', (title,))
                        result = cursor.fetchone()
                        if result:
                            paper_id = result[0]
                    
                    if paper_id:
                        # Preparar dados da an√°lise
                        analysis_results = {
                            'relevance_score': float(row.get('relevance_score', 0)),
                            'comp_techniques': row.get('comp_techniques'),
                            'study_type': row.get('study_type'),
                            'eval_methods': row.get('eval_methods'),
                            'selection_stage': row.get('selection_stage')
                        }
                        
                        # Salvar an√°lise com ID real
                        db_manager.save_analysis(
                            paper_id=paper_id,
                            analysis_type='relevance_scoring',
                            results=analysis_results,
                            confidence=float(row.get('relevance_score', 0))/10.0
                        )
                        analysis_count += 1
            
            conn.close()
            print(f"üìä Salvou {analysis_count} an√°lises no banco de dados")
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao salvar an√°lises: {e}")
        
        # ‚úÖ CORRE√á√ÉO: Log da execu√ß√£o do pipeline na tabela searches
        try:
            search_log = {
                'apis_used': apis,
                'limit_per_query': limit_per_query,
                'min_score': min_score,
                'total_papers': len(pipeline.results),
                'included_papers': len(pipeline.results[pipeline.results['selection_stage'] == 'included'])
            }
            
            db_manager.save_search_log(
                query_summary=f"Pipeline completo: {len(apis)} APIs, {limit_per_query} papers/query",
                results_summary=search_log
            )
            print(f"üìù Log da execu√ß√£o salvo na tabela searches")
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao salvar log: {e}")
        
        # Exportar resultados
        # Recarregar do banco para garantir consist√™ncia com a fonte de verdade
        try:
            df_db = read_papers(cfg)
            if not df_db.empty:
                pipeline.results = df_db
        except Exception as e:
            logger.warning(f"Falha ao recarregar dados do banco antes de exportar: {e}")

        export_files = pipeline.export_results()
        
        print(f"\n‚úÖ Pipeline conclu√≠do!")
        print(f"üìÑ Total processado: {len(pipeline.results)} papers")
        
        if "selection_stage" in pipeline.results.columns:
            stage_counts = pipeline.results["selection_stage"].value_counts()
            print("\nüìä Resultados por est√°gio:")
            for stage, count in stage_counts.items():
                print(f"  {stage}: {count}")
        
        # Mostrar arquivos gerados
        if isinstance(export_files, dict):
            print(f"\nüìÅ Arquivos gerados:")
            for file_type, path in export_files.items():
                if isinstance(path, list):
                    print(f"  {file_type}: {len(path)} arquivos")
                else:
                    print(f"  {file_type}: {path}")
    
    except Exception as e:
        print(f"‚ùå Erro durante execu√ß√£o: {e}")
        logger.error(f"Pipeline failed: {e}", exc_info=True)

def cmd_export(ns: argparse.Namespace) -> None:
    cfg = load_config()
    df = read_papers(cfg)
    
    if df.empty:
        print("‚ùå Nenhum paper encontrado no banco")
        return
    
    from .exports.excel import export_complete_review
    
    # Calcular estat√≠sticas PRISMA para visualiza√ß√µes
    stats = None
    try:
        if "selection_stage" in df.columns and "status" in df.columns:
            identification = int(len(df))
            
            # Debug: distribui√ß√£o de est√°gios
            stage_dist = df["selection_stage"].value_counts().to_dict()
            status_dist = df["status"].value_counts().to_dict()
            logger.info(f"üìä Distribui√ß√£o est√°gios: {stage_dist}")
            logger.info(f"üìã Distribui√ß√£o status: {status_dist}")
            
            # Screening: todos que passaram triagem
            screening = int(((df["selection_stage"] == "screening") & (df["status"] != "excluded")).sum())
            screening_excluded = int(((df["selection_stage"] == "screening") & (df["status"] == "excluded")).sum())
            
            # Eligibility: todos que passaram elegibilidade
            eligibility = int(((df["selection_stage"] == "eligibility") & (df["status"] != "excluded")).sum())
            eligibility_excluded = int(((df["selection_stage"] == "eligibility") & (df["status"] == "excluded")).sum())
            
            # Included: papers finais
            included = int(((df["selection_stage"] == "included") & (df["status"] == "included")).sum())
            
            # Se n√£o h√° screening no banco, assumir que todos foram direto para eligibility
            # Nesse caso, screening = identification (todos passaram triagem implicitamente)
            if screening == 0 and screening_excluded == 0:
                screening = identification
                logger.warning("‚ö†Ô∏è Nenhum registro em 'screening' - assumindo todos passaram triagem")
            
            logger.info(f"üìà PRISMA calculado: ident={identification}, screen={screening}, "
                       f"screen_excl={screening_excluded}, elig={eligibility}, "
                       f"elig_excl={eligibility_excluded}, incl={included}")
            
            stats = {
                "identification": identification,
                "duplicates_removed": 0,  # duplicatas tratadas antes
                "screening": screening,
                "screening_excluded": screening_excluded,
                "eligibility": eligibility,
                "eligibility_excluded": eligibility_excluded,
                "included": included,
            }
    except Exception as e:
        logger.error(f"‚ùå Erro ao calcular PRISMA stats: {e}", exc_info=True)
    
    output_dir = Path(ns.output) if hasattr(ns, 'output') and ns.output else None
    files = export_complete_review(df, stats=stats, output_dir=output_dir)
    
    print("üìä Exporta√ß√£o completa realizada:")
    for file_type, path in files.items():
        if isinstance(path, list):
            print(f"  {file_type}: {len(path)} arquivos")
        else:
            print(f"  {file_type}: {path}")


def cmd_normalize_prisma(_: argparse.Namespace) -> None:
    cfg = load_config()
    db = DatabaseManager(cfg)
    res = db.normalize_prisma_stages()
    print("üîß Normaliza√ß√£o PRISMA conclu√≠da:")
    print(f"  screening_excluded -> screening/status=excluded: {res['screening']}")
    print(f"  eligibility_excluded -> eligibility/status=excluded: {res['eligibility']}")



def build_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(
        prog="ccw-rs",
        description="CLI da revis√£o sistem√°tica com banco SQLite e visualiza√ß√µes",
    )
    sub = p.add_subparsers(dest="command", required=True)

    # Comando init-db
    p_init = sub.add_parser("init-db", help="Inicializa o banco de dados SQLite")
    p_init.set_defaults(func=cmd_init_db)

    # Comando show
    p_show = sub.add_parser("show", help="Mostra amostra dos registros na tabela papers")
    p_show.set_defaults(func=cmd_show)

    # Comando stats
    p_stats = sub.add_parser("stats", help="Mostra estat√≠sticas do banco de dados")
    p_stats.set_defaults(func=cmd_stats)

    # Comando import-csv
    p_import = sub.add_parser("import-csv", help="Importa resultados de busca (CSV) para o banco")
    p_import.add_argument("csv", help="Caminho para o arquivo CSV com colunas incluindo 'doi'")
    p_import.set_defaults(func=cmd_import_csv)

    # Comando run-pipeline
    p_pipeline = sub.add_parser("run-pipeline", help="Executa o pipeline completo de revis√£o sistem√°tica")
    p_pipeline.add_argument("--min-score", type=float, default=4.0, 
                           help="Score m√≠nimo de relev√¢ncia (padr√£o: 4.0)")
    p_pipeline.add_argument("--apis", nargs="+", 
                           choices=["semantic_scholar", "openalex", "crossref", "core"],
                           default=["semantic_scholar", "openalex", "crossref", "core"],
                           help="APIs a usar na busca")
    p_pipeline.add_argument("--limit-per-query", type=int, default=50,
                           help="Limite de resultados por query por API")
    p_pipeline.set_defaults(func=cmd_run_pipeline)

    # Comando export
    p_export = sub.add_parser("export", help="Exporta dados com relat√≥rios e visualiza√ß√µes")
    p_export.add_argument("-o", "--output", help="Diret√≥rio de sa√≠da")
    p_export.set_defaults(func=cmd_export)

    # Comando normalize-prisma
    p_norm = sub.add_parser("normalize-prisma", help="Normaliza est√°gios PRISMA legados no banco")
    p_norm.set_defaults(func=cmd_normalize_prisma)

    return p


def main(argv: list[str] | None = None) -> None:
    parser = build_parser()
    ns = parser.parse_args(argv)
    ns.func(ns)


if __name__ == "__main__":
    main()
